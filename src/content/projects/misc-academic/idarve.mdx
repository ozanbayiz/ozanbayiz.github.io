## Abstract

Machine learning systems, especially modern Vision-Language Models (VLMs), can inherit and amplify societal biases related to race, gender, and age (Florence‑2 VLM; FairFace dataset). To build fairer and more trustworthy AI, we move beyond merely measuring bias to analyzing how it is represented internally in the vision encoder. In this project, we investigate Microsoft's Florence‑2 vision encoder using a combination of linear probing and Sparse Autoencoders (SAEs) to identify and visualize which internal features encode sensitive demographic attributes. Our findings show that some features align with demographic labels, while others often rely on contextual or cultural markers (e.g., headscarves, beards), illuminating one mechanism by which bias can emerge in downstream tasks.

<SectionDivider />

## The Challenge: Seeing Inside the "Black Box"

Large-scale vision models are often treated as "black boxes," making it difficult to understand why they produce particular outputs. When these models leverage sensitive attributes—explicitly or via proxies—it can perpetuate stereotypes and harm user trust. Our goal is to dissect internal representations to clarify how, and where, demographic information is encoded within the visual pathway of the model.

<SectionDivider />

## Our Approach: A Three-Step Methodological Pipeline

### Step 1: Confirming the Signal with Linear Probing

We first test whether demographic information is linearly decodable from the vision encoder's activations. We extract patch-level features, average-pool to image vectors, and train simple linear probes for race, gender, and age. The race probe achieves 62.15% accuracy on a 7-class task (vs. 14.3% random baseline), confirming that demographic information is strongly present and justifying a deeper analysis (Florence‑2; FairFace).

![Linear probing pipeline](/projects/idarve/LP_training.png)
> Figure 1: Linear probing pipeline. Images are encoded by the vision encoder; pooled features feed linear classifiers for demographic prediction.

### Step 2: Decomposing Activations with Sparse Autoencoders (SAEs)

To understand the structure of this information, we train an overcomplete SAE on patch-level activations, preserving spatial detail for more fine-grained interpretability. The SAE learns a dictionary of sparse, interpretable features (Sparse Dictionary Features, SDFs) that can be visualized and analyzed at the image and patch levels.

![Patch-level SAE pipeline](/projects/idarve/SAE_training.png)
> Figure 2: Patch-level SAE. Patch features are compressed into sparse, interpretable activations and then used to reconstruct the original features.

### Step 3: Identifying and Filtering Race‑Correlated Features

From the learned dictionary, we apply a filtering pipeline to surface SDFs most relevant to race:

- Activation frequency: features that frequently activate for a specific group
- Mean activation strength: features that activate strongly (not just often)
- Label entropy: features with high specificity to a single group

<SectionDivider />

## Key Findings: Models Learn Proxies and Shortcuts

The analysis reveals a mixture of signals. Some features align strongly with demographic labels, while others are entangled with unrelated concepts. Most notably, several features act as proxies for demographic categories by firing on contextual or cultural markers (e.g., headscarves or beards), suggesting shortcut learning that can manifest as bias in downstream tasks.

<ImageGrid>
![Headscarves activate a race‑associated feature](/projects/idarve/stereotype_feature1.png)
![Beards activate a race‑associated feature](/projects/idarve/stereotype_feature2.png)
</ImageGrid>
> Figure 3: Examples of proxy features. Left: a feature strongly activates on headscarves; Right: another activates on beards. These contextual cues act as demographic proxies rather than intrinsic facial characteristics.

<SectionDivider />

## Limitations & Future Directions

- Unsupervised discovery risk: some learned features remain entangled and not cleanly demographic-specific
- Proxy sensitivity: contextual cues may reflect dataset correlations rather than intrinsic attributes
- Scope: analysis focused on race in FairFace; broader datasets and attributes are needed
- Causality not established: correlations do not prove that features cause biased behavior

Future work includes: (1) causal interventions (feature editing/pruning/ablation) to test whether identified features drive biased outcomes, (2) semi‑supervised SAEs guided by concept targets, and (3) evaluation of robustness and generalization across datasets and VLMs.

<SectionDivider />

## Resources

- Read the full paper (PDF): [writeup.pdf](/idarve/writeup.pdf)
- Code: https://github.com/ozanbayiz/idarve


export const meta = {
  layout: 'project'
}


