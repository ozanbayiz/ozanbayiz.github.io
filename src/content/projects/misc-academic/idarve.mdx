## Abstract

Machine learning systems are prone to exhibiting misaligned behavior when inputs include sensitive attributes such as race, age or gender. 
Understanding how neural networks encode these attributes is essential for achieving fairness, transparency, interpretability, and overall alignment. 
This project investigates activations of the vision encoder (VE) of a vision language model (VLM) – in this case a vision transformer (ViT) – when provided input of images of people of different races, genders, and ages. 
We specifically use Microsoft’s Florence-2-base model, and perform the following analyses: First, we determined the degree to which the VE’s output activations contain features that are relevant to encoding demographic information by training linear probes to classify VE encoded images of people based on race, gender, and age. 
We then train sparse autoencoders (SAEs) to decompose these activations into human-interpretable latent features. 
Finally, we identify sparse dictionary features (SDFs) that are correlated with particular demographic features, providing insights into how these attributes might be represented.

<SectionDivider />

## The Challenge: Seeing Inside the "Black Box"

Large-scale vision models are often treated as "black boxes," making it difficult to understand why they produce particular outputs. When these models leverage sensitive attributes—explicitly or via proxies—it can perpetuate stereotypes and harm user trust. Our goal is to dissect internal representations to clarify how, and where, demographic information is encoded within the visual pathway of the model.

<SectionDivider />

## Our Approach: A Three-Step Methodological Pipeline

### Step 1: Confirming the Signal with Linear Probing

We first test whether demographic information is linearly decodable from the vision encoder's activations. We extract patch-level features, average-pool to image vectors, and train simple linear probes for race, gender, and age. The race probe achieves 62.15% accuracy on a 7-class task (vs. 14.3% random baseline), confirming that demographic information is strongly present and justifying a deeper analysis (Florence‑2; FairFace).

![Linear probing pipeline](/projects/idarve/LP_training.png)
> Figure 1: Linear probing pipeline. Images are encoded by the vision encoder; pooled features feed linear classifiers for demographic prediction.

### Step 2: Decomposing Activations with Sparse Autoencoders (SAEs)

To understand the structure of this information, we train an overcomplete SAE on patch-level activations, preserving spatial detail for more fine-grained interpretability. The SAE learns a dictionary of sparse, interpretable features (Sparse Dictionary Features, SDFs) that can be visualized and analyzed at the image and patch levels.

![Patch-level SAE pipeline](/projects/idarve/SAE_training.png)
> Figure 2: Patch-level SAE. Patch features are compressed into sparse, interpretable activations and then used to reconstruct the original features.

### Step 3: Identifying and Filtering Race‑Correlated Features

From the learned dictionary, we apply a filtering pipeline to surface SDFs most relevant to race:

- Activation frequency: features that frequently activate for a specific group
- Mean activation strength: features that activate strongly (not just often)
- Label entropy: features with high specificity to a single group

<SectionDivider />

## Key Findings: Models Learn Proxies and Shortcuts

The analysis reveals a mixture of signals. Some features align strongly with demographic labels, while others are entangled with unrelated concepts. Most notably, several features act as proxies for demographic categories by firing on contextual or cultural markers (e.g., headscarves or beards), suggesting shortcut learning that can manifest as bias in downstream tasks.

<ImageGrid>
![Headscarves activate a race‑associated feature](/projects/idarve/stereotype_feature1.png)
![Beards activate a race‑associated feature](/projects/idarve/stereotype_feature2.png)
</ImageGrid>
> Figure 3: Examples of proxy features. Left: a feature strongly activates on headscarves; Right: another activates on beards. These contextual cues act as demographic proxies rather than intrinsic facial characteristics.

<SectionDivider />

## Limitations & Future Directions

- Unsupervised discovery risk: some learned features remain entangled and not cleanly demographic-specific
- Proxy sensitivity: contextual cues may reflect dataset correlations rather than intrinsic attributes
- Scope: analysis focused on race in FairFace; broader datasets and attributes are needed
- Causality not established: correlations do not prove that features cause biased behavior

Future work includes: (1) causal interventions (feature editing/pruning/ablation) to test whether identified features drive biased outcomes, (2) semi‑supervised SAEs guided by concept targets, and (3) evaluation of robustness and generalization across datasets and VLMs.

<SectionDivider />



export const meta = {
  layout: 'project'
}


