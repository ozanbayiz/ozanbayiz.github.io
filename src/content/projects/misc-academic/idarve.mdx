---
title: "Investigating Demographic Attribute Representation in Vision Encoders"
shortTitle: "IDARVE"
description: "Probed Florence‑2's DaViT vision encoder with linear classifiers and trained patch‑level SAEs to discover interpretable sparse dictionary features (SDFs)."
thumbnail: "/projects/idarve/thumbnail.png"
heroImage: "/projects/idarve/thumbnail.png"
gitUrl: "https://github.com/ozanbayiz/idarve"
date: "2025-05-11"
pdfUrl: "https://drive.google.com/file/d/1GGZpM5Wz_wwaz6jEWMMBS_Ixga-qs3XV/view?usp=drive_link"
featured: true
---

UC Berkeley, Spring 2025

<SectionDivider />

<a id="abstract" />
## Abstract

Machine learning systems can exhibit misaligned behavior when inputs include sensitive attributes such as race, age, or gender. 
Understanding how neural networks encode these attributes is important for fairness and interpretability. 
This project investigates activations of the vision encoder (VE) of a Vision Language Model (VLM), specifically a vision transformer (ViT), when processing images of people of different races, genders, and ages. 
We use Microsoft's Florence-2-base model and perform the following analyses: first, we train linear probes to classify VE-encoded images based on race, gender, and age, measuring the degree to which output activations encode demographic information. 
We then train sparse autoencoders (SAEs) to decompose these activations into interpretable latent features. 
Finally, we identify sparse dictionary features (SDFs) correlated with particular demographic attributes, providing insights into how these attributes are represented.

<SectionDivider />

<a id="motivation" />
## Motivation

Vision Language Models (VLMs) like 
<ExternalLink href="https://arxiv.org/abs/2311.06242"> Microsoft Florence-2</ExternalLink>
(<ExternalLink href="https://huggingface.co/microsoft/Florence-2-base">huggingface model card</ExternalLink>)
integrate visual and textual information for tasks such as image captioning
and visual question answering. 

These models often inherit societal biases related to sensitive attributes like race, gender, and age from their training data,
causing them to propagate harmful stereotypes.

Should VLMs' internal representation of a person be agnostic to their age, race, and/or gender identity? 
If so, is it possible to steer this latent representation away from harmful biases in the training data?

Understanding how visually observable demographic attributes are represented
within the activation space of a VLM will provide useful insights into aligning ML behaviors
to constructive human preferences.

<SectionDivider />

<a id="fairface" />
## FairFace Dataset

The 
<ExternalLink href="https://arxiv.org/abs/1908.04913">FairFace dataset</ExternalLink>
(<ExternalLink href="https://huggingface.co/datasets/HuggingFaceM4/FairFace">huggingface dataset card</ExternalLink>)
 is a publicly available collection of face images created to address and reduce racial bias in computer vision, especially in facial recognition systems.

FairFace contains over 100,000 face images, annotated by age, gender, and race. 
The dataset includes seven race groups: 
White, Black, East Asian, Southeast Asian, Indian, Middle Eastern, and Latino.
FairFace aims for balanced representation across races, ages, and genders,
unlike many existing datasets that are skewed toward certain demographics.

We chose FairFace due to this explicit balancing, 
enabling us to overcome entanglement of attributes
and control for specific attributes using stratified sampling.

<SectionDivider />

<a id="methodology" />
## Methodology

### Linear Probing

First, we test if demographic information can be linearly decoded from the Vision Encoder's (VE) activations.
Using Florence-2's VE, we encode images to extract patch-level features:

$$
\text{VE}(X_{\text{img}}) = Z_{\text{img}} \in \mathbb{R}^{N_{\text{patches}} \times D_{\text{patch}}}
$$

Due to compute constraints,
we could not train linear probes directly on flattened image features.
We computed the mean across the patch dimension, yielding a single vector for each image:

$$
\text{mean}(Z_{\text{img}}) = \overline{z_{\text{img}}} \in \mathbb{R}^{D_{\text{patch}}}
$$

We then train simple linear probes to predict the race, age, and gender labels of the face in the encoded image.

<Figure src="/projects/idarve/LP_training.png" alt="Linear probing pipeline" caption="Linear probing pipeline. Images are encoded by the vision encoder; pooled features feed linear classifiers for demographic prediction." />

After training, we evaluated the performance of linear probes on a validation dataset. 

<Figure src="/projects/idarve/LP_performance.png" alt="Linear probe validation results" caption="Linear probe performance on validation set." />

Even after averaging across patches and losing spatial information,
the trained probes predicted demographic attribute labels with accuracy considerably higher than random chance.
This indicated that the patch-level features in Florence-2's VE encode demographic information,
motivating the next stage of analysis.

---

### Decomposing Activations with Sparse Autoencoders (SAEs)

We trained a SAE on patch-level activations.

<Figure src="/projects/idarve/SAE_training.png" alt="SAE training pipeline" caption="Patch-level SAE. Patch features are compressed into sparse, interpretable activations and then used to reconstruct the original feature." />

The SAE learns a dictionary of sparse, interpretable features (Sparse Dictionary Features, SDFs)
that can be analyzed to identify correlations between demographic labels and particular patterns in the activation space.

---

### Identifying Relevant SDFs

After training our SAE, we identified which SDFs were correlated to which particular labels $l \in \{ l_1, ... l_c \} = \mathcal{L}$.

We follow the multi-stage process outlined in <ExternalLink href="https://arxiv.org/abs/2412.05276">**Lim et al. (2024)**</ExternalLink>
to systematically narrow down the SDF dictionary to focused lists of candidates.

#### 1. Activation frequency

The first step identifies SDFs that are commonly activated when processing
images belonging to a particular label. 
For each SDF and each label, 
we calculate the proportion of datapoints with that label for which the SDF's aggregated activation exceeds a threshold $\tau > 0$. 

This yields a frequency score indicating how often the feature "fires" for that label.
We select the top $k_1$ SDFs with the highest activation frequency for each label, 
forming an initial pool of candidates.

#### 2. Mean activation strength

Next, we filter for features that activate strongly when present.
For each label, 
we consider its $k_1$ candidate SDFs from the previous step 
and calculate the mean activation value for each, 
averaging only across images within the group where the SDF was active (activation $> \tau$). 

We retain the top $k_2$ SDFs per group ranked by this mean activation value, 
keeping SDFs that are both frequent and strongly activated.

#### 3. Label Entropy

A feature might be frequent and strong within one group but also activate for other groups.
To isolate features more uniquely associated with a target group, we calculate the label
entropy for each of the remaining $k_2$ candidate SDFs per group.

For each SDF $f_i$, we:
1. Compute the sum of $f_i$'s activation across all images with a given label:

$$ 
\text{label activation}(f_i, \ l) = \sum_{\text{img}_k \in l} \text{activation}( f_i, x_{\text{img}_k})
$$

2. Normalize these sums to obtain a probability distribution $\pi_{f_i, l_j}$, 
indicating how likely a feature $f_i$ is to fire for an image with label $l_j$:

$$
\pi_{f_i, l_j} = \frac{ \text{label activation} (f_i, l_j) }{\sum_{l_k \in \mathcal{L} } \text{label activation}(f_i, l_k) }
$$

3. Compute the Shannon entropy using these distributions:

$$
\text{entropy}(f_i) = - \sum_{l_j \in \mathcal{L} } \pi_{f_i,l_j} \log \pi_{f_i,l_j}
$$

Low entropy means the SDF's activation is concentrated within one or a few labels, indicating high specificity.
High entropy means the feature is broadly active across many groups.

We retain the top $k_3$ SDFs per label with the lowest label entropy, 
yielding the final set of candidate SDFs most specific to each label.

<SectionDivider />

<a id="findings" />
## Findings: VEs "See" Cultural Markers

Due to time constraints, we were only able to perform the SDF identification process for 
the race attribute of our dataset.

The analysis revealed a mixture of signals.
Some features align strongly with demographic labels, 
while others are entangled with unrelated concepts.

<Figure src="/projects/idarve/neckbeard_feature.jpg" alt="A feature associated with sunglasses" caption="A feature associated with images of people wearing sunglasses."/>

We also found features aligned to cultural markers such as headscarves and beards.

<ImageGrid>
<Figure src="/projects/idarve/stereotype_feature1.png" alt="Headscarves activate a race-associated feature" caption="A feature strongly activates on headscarves, acting as a demographic proxy." />
<Figure src="/projects/idarve/stereotype_feature2.png" alt="Beards activate a race-associated feature" caption="A feature activates on beards, also acting as a demographic proxy." />
</ImageGrid>

Training datasets for contemporary VLMs will contain correlations between certain visual features and demographic attributes.

<SectionDivider />

<a id="limitations" />
## Limitations & Future Directions

- Unsupervised discovery risk: some learned features remain entangled and not cleanly demographic-specific.
- Proxy sensitivity: contextual cues may reflect dataset correlations rather than intrinsic attributes.
- Scope: analysis focused on race in FairFace; controlling for age/gender could guide the identification of more concentrated SDFs.
- Causality not established: correlations do not prove that features contribute to demographic attribute understanding in the language model.

We think future work will involve:

1. Causal interventions (feature editing/pruning/ablation) to test whether identified features drive biased outcomes

2. Semi-supervised SAEs guided by concept targets

3. Evaluation of robustness and generalization across datasets and VLMs.
