<a id="part-2" />
## Part 1: Training a Single-Step Denoising U-Net

<a id="part-2-1" />
### 1.1 Implementing the U-Net

I implemented an unconditional U-Net with the architecture provided in the project spec.

<Figure src="/projects/cs180/proj5b/backbones/unconditional_arch.png" alt="Unconditional U-Net architecture" caption="Figure 1. Unconditional U-Net architecture." />
<Figure src="/projects/cs180/proj5b/backbones/atomic_ops_new.png" alt="Atomic operations block" caption="Figure 2. Blocks that compose the backbone." />

<a id="part-2-2" />
### 1.2 Using the U-Net to Train a Denoiser

Given a clean image, $x$, I noised it to get $z = x + \sigma \epsilon$, where $\epsilon \sim \mathcal{N}(0, 1)$, and $\sigma \in \{0.0, 0.2, 0.4, 0.6, 0.8, 1.0\}$.

<Figure src="/projects/cs180/proj5b/uncond/noising_process.png" alt="Noising process" caption="Figure 3. Forward noising process for constructing $(\mathbf{x}, \mathbf{z})$ pairs." />

Using this, I created a dataloader which produced batches of the form $(x, z)$ with $ \sigma = 0.5 $.

<Figure src="/projects/cs180/proj5b/uncond/noising_dataloader.png" alt="Noising dataloader" caption="Figure 4. Noising dataloader output for $\sigma = 0.5$." />

<a id="part-2-2-1" />
#### 1.2.1 Training

I trained the U-Net to predict the noise $\epsilon_{\sigma} = z - x$, and optimized it against the loss $\| \epsilon_{\sigma}  - \hat{\epsilon}\|^2 $

<Figure src="/projects/cs180/proj5b/uncond/uncond_unet_6_epochs.png" alt="Unconditional U-Net training loss" caption="Figure 5. Training loss for the unconditional U-Net." />

Here are my results sampling after 1 and 5 training epochs.

<ImageGrid>

<Figure src="/projects/cs180/proj5b/uncond/1_epoch_samples.png" alt="1 epoch samples (unconditional)" caption="Figure 6. Samples after 1 epoch of training (unconditional)." />

<Figure src="/projects/cs180/proj5b/uncond/5_epoch_samples.png" alt="5 epoch samples (unconditional)" caption="Figure 7. Samples after 5 epochs of training (unconditional)." />

</ImageGrid>
Pretty good!

<a id="part-2-2-2" />
#### 1.2.2 Out-of-Distribution Testing

Then I tried denoising images with $\sigma \neq 0.5$.

<Figure src="/projects/cs180/proj5b/uncond/out_of_dist_samples.png" alt="Out-of-distribution samples" caption="Figure 8. Denoising performance for out-of-distribution $\sigma$ values." />

Still, not bad!

---

<a id="part-3" />
## Part 2: Training a Diffusion Model

<a id="part-3-1" />
### 2.1 Adding Time Conditioning to U-Net

I followed the spec and implemented time conditioning.

<Figure src="/projects/cs180/proj5b/backbones/conditional_arch.png" alt="Time-conditioned U-Net architecture" caption="Figure 9. Time-conditioned U-Net architecture." />

<Figure src="/projects/cs180/proj5b/backbones/fc_long.png" alt="Fully connected block (time embedding)" caption="Figure 10. Fully connected block used for time embeddings." />

As recommended in the spec, I used a hidden dimension of 64 and a batch size of 128. 
For optimization, I used Adam with a learning rate of 0.001 and an exponential learning rate scheduler with $\gamma = 0.9$.

<a id="part-3-2" />
### 2.2 Training the U-Net

I used this algorithm for training the Unconditional U-Net
<div className='math-wrap text-[clamp(0.9rem,2.4vw,1.05rem)]'>
$$
\begin{aligned}
&\textbf{Training}\\
&\text{Precompute } \bar{\alpha}\\
&\textbf{repeat}\\
&\quad \mathbf{x}_0 \sim \text{clean image from training set}\\
&\quad t \sim \mathrm{Uniform}(\{1,\ldots,T\})\\
&\quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})\\
&\quad \mathbf{x}_t \gets \sqrt{\bar{\alpha}_t}\,\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\epsilon}\\
&\quad \hat{\boldsymbol{\epsilon}} \gets \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\\
&\quad \text{update } \theta \text{ to minimize } \lVert \boldsymbol{\epsilon}-\hat{\boldsymbol{\epsilon}}\rVert^2\\
&\textbf{until happy}
\end{aligned}
$$
</div>

I plotted the training loss $\| \epsilon - \hat{\epsilon} \|^2 $ for the time-conditioned U-Net during 20 epochs.

<Figure src="/projects/cs180/proj5b/time_cond/training_loss.png" alt="Time-conditioned U-Net training loss" caption="Figure 11. Training loss for the time-conditioned U-Net (20 epochs)." />

<a id="part-3-3" />
### 2.3 Sampling from the U-Net

After training, I used this algorithm to sample from the U-Net.

<div className='math-wrap text-[clamp(0.9rem,2.4vw,1.05rem)]'>
$$
\begin{aligned}
&\textbf{Sampling}\\
&\text{Precompute } \boldsymbol{\beta},\, \boldsymbol{\alpha},\, \bar{\boldsymbol{\alpha}}\\
&\mathbf{x}_T \sim \mathcal{N}(\mathbf{0},\mathbf{I})\\
&\textbf{for } t=T \text{ to } 1 \text{ step } -1\\
&\quad \mathbf{z} \sim \mathcal{N}(\mathbf{0},\mathbf{I}) \text{ if } t>1,\ \text{else } \mathbf{z} \gets \mathbf{0}\\
&\quad \hat{\mathbf{x}}_0 \gets \frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)\right)\\
&\quad \mathbf{x}_{t-1} \gets \frac{\sqrt{\bar{\alpha}_{t-1}}\,\beta_t}{1-\bar{\alpha}_t}\,\hat{\mathbf{x}}_0 + \frac{\sqrt{\alpha_t}\,\bigl(1-\bar{\alpha}_{t-1}\bigr)}{1-\bar{\alpha}_t}\,\mathbf{x}_t + \sqrt{\beta_t}\,\mathbf{z}\\
&\textbf{end for}\\
&\textbf{return } \mathbf{x}_0
\end{aligned}
$$
</div>

I sampled after 5 and 20 epochs of training.

 
<ImageGrid>
<Figure src="/projects/cs180/proj5b/time_cond/5_epoch_samples.png" alt="5 epoch samples (time-conditioned)" caption="Figure 12. Samples after 5 epochs (time-conditioned)." />
<Figure src="/projects/cs180/proj5b/time_cond/20_epoch_samples.png" alt="20 epoch samples (time-conditioned)" caption="Figure 13. Samples after 20 epochs (time-conditioned)." />
</ImageGrid>
 
<a id="part-3-4" />
### 2.4 Adding Class-Conditioning to U-Net

I implemented class conditioning as specified in the spec. I used the same hyperparameters as in part 2.2, and got the following training loss for 20 epochs.

Here is the algorithm I used for class-conditioned training.

$$
\begin{aligned}
&\textbf{Class-Conditioned Training}\\
&\text{Precompute } \bar{\alpha}\\
&\textbf{repeat}\\
&\quad (\mathbf{x}_0, c) \sim \text{clean image and label from training set}\\
&\quad \text{make } c \text{ one-hot};\ \text{with probability } p_{\text{uncond}} \text{ set } c \gets \mathbf{0}\\
&\quad t \sim \mathrm{Uniform}(\{1,\ldots,T\})\\
&\quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})\\
&\quad \mathbf{x}_t \gets \sqrt{\bar{\alpha}_t}\,\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\epsilon}\\
&\quad \hat{\boldsymbol{\epsilon}} \gets \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, c)\\
&\quad \text{update } \theta \text{ to minimize } \lVert \boldsymbol{\epsilon}-\hat{\boldsymbol{\epsilon}}\rVert^2\\
&\textbf{until happy}
\end{aligned}
$$

Once again, I plotted the training loss $\| \epsilon - \hat{\epsilon} \|^2$

<Figure src="/projects/cs180/proj5b/class_cond/training_loss.png" alt="Class-conditioned U-Net training loss" caption="Figure 14. Training loss for the class-conditioned U-Net (20 epochs)." />

<a id="part-3-5" />
### 2.5 Sampling from the Class-Conditioned U-Net

After training, I used this algorithm to sample from the class-conditioned U-Net

<div className='math-wrap text-[clamp(0.9rem,2.4vw,1.05rem)]'>
$$
\begin{aligned}
&\textbf{Class-Conditioned Sampling}\\
&\text{input: one-hot } c,\ \text{guidance scale } \gamma\\
&\mathbf{x}_T \sim \mathcal{N}(\mathbf{0},\mathbf{I})\\
&\textbf{for } t=T \text{ to } 1 \text{ step } -1\\
&\quad \mathbf{z} \sim \mathcal{N}(\mathbf{0},\mathbf{I}) \text{ if } t>1,\ \text{else } \mathbf{z} \gets \mathbf{0}\\
&\quad \boldsymbol{\epsilon}_u \gets \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \mathbf{0})\\
&\quad \boldsymbol{\epsilon}_c \gets \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, c)\\
&\quad \boldsymbol{\epsilon} \gets \boldsymbol{\epsilon}_u + \gamma\bigl(\boldsymbol{\epsilon}_c - \boldsymbol{\epsilon}_u\bigr)\\
&\quad \hat{\mathbf{x}}_0 \gets \frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\epsilon}\right)\\
&\quad \mathbf{x}_{t-1} \gets \frac{\sqrt{\bar{\alpha}_{t-1}}\,\beta_t}{1-\bar{\alpha}_t}\,\hat{\mathbf{x}}_0 + \frac{\sqrt{\alpha_t}\,\bigl(1-\bar{\alpha}_{t-1}\bigr)}{1-\bar{\alpha}_t}\,\mathbf{x}_t + \sqrt{\beta_t}\,\mathbf{z}\\
&\textbf{end for}\\
&\textbf{return } \mathbf{x}_0
\end{aligned}
$$
</div>

I sampled after 5 and 20 epochs of training.

<ImageGrid>
<Figure src="/projects/cs180/proj5b/class_cond/5_epoch_samples.png" alt="5 epoch samples (class-conditioned)" caption="Figure 15. Samples after 5 epochs (class-conditioned)." />
<Figure src="/projects/cs180/proj5b/class_cond/20_epoch_samples.png" alt="20 epoch samples (class-conditioned)" caption="Figure 16. Samples after 20 epochs (class-conditioned)." />
</ImageGrid>

But something was wrong. My handwriting is pretty bad too, but like, come on man.

After closer inspection, I noticed that I was missing normalization layers in my convolutional blocks. So I fixed this bug and re-launched training.


<Figure src="/projects/cs180/proj5b/ddpm_samples_training.gif" alt="DDPM sampling progression across training epochs" caption="Figure 17. DDPM sampling progression as training proceeds." />

My samples were much cleaner this time around. *The Lesson?* normalization layers are pretty important.

<ImageGrid>
<Figure src="/projects/cs180/proj5b/ddpm_samples.gif" alt="DDPM sampling animation from noise with the trained model" caption="Figure 18. DDPM sampling process." />
<Figure src="/projects/cs180/proj5b/ddpm_samples.png" alt="DDPM generated samples (grid view)" caption="Figure 19.  DDPM samples after training." />
</ImageGrid>


<a id="appendix-rf" />
## Bonus: Rectified Flow

<ImageGrid>
<Figure src="/projects/cs180/proj5b/rf_samples.gif" alt="Rectified Flow sampling animation from noise" caption="Figure 20. Rectified Flow sampling process." />
<Figure src="/projects/cs180/proj5b/rf_samples.png" alt="Rectified Flow generated samples (grid view)" caption="Figure 21. Rectified Flow samples." />
</ImageGrid>

{/* summary-of-edits
- Standardized terminology to "U-Net" throughout and corrected grammar/punctuation.
- Fixed broken image tags/paths and added missing attributes; removed stray markup.
- Converted all images to use `Figure` with sequential captions (Figure 1–16) and concise, descriptive alt text.
- Resolved minor inconsistencies (e.g., epoch counts) and improved clarity without altering technical content.
*/}