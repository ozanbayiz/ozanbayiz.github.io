---
title: "Neural Radiance Field!"
shortTitle: "NeRF"
description: "Fitting a 2D neural field and a NeRF from multi-view images, including ray generation, sampling, NeRF architecture, and volume rendering results."
thumbnail: "/projects/cs180/proj6/agony.jpeg"
heroImage: "/projects/cs180/proj6/agony.jpeg"
gitUrl: "https://github.com/ozanbayiz/cs180"
date: "2024-12-13"
---

CS 180: Intro to Computer Vision and Computational Photography, Project 6, Fall 2024

<SectionDivider />

Neural Radiance Fields (NeRF) synthesize novel views of 3D scenes by optimizing a continuous volumetric scene function. This project implements a NeRF from scratch, starting with fitting a 2D neural field to an image and progressing to a full 3D NeRF trained on multi-view data.

A NeRF represents a scene as a continuous volumetric function parameterized by an MLP. The network takes a 5D input (spatial location $(x, y, z)$ and viewing direction $(\theta, \phi)$) and outputs the volume density $\sigma$ and view-dependent emitted color $c$ at that location.

To render a view:
1. **Ray Casting**: Rays are marched from the camera center through each pixel into the scene.
2. **Sampling**: Points are sampled along each ray.
3. **Querying**: The MLP predicts density and color for each sampled point.
4. **Volume Rendering**: These values are composited using differentiable volume rendering to compute the final pixel color.

The network learns a 3D representation by minimizing the error between rendered pixels and ground truth pixels from input images.

<SectionDivider />

<a id="2d-neural-field" />
## Part 1: Fit a Neural Field to a 2D Image

I first implemented a neural field to represent a 2D image, where the network learns a mapping from pixel coordinates $(x, y)$ to RGB colors $(r, g, b)$.

---

### Positional Encoding

Raw low-dimensional coordinates are mapped to a higher-dimensional space using sinusoidal positional encoding. For a scalar input $p$, the encoding with $L$ frequency bands is:

$$
\gamma(p) = \left[\sin(2^0 \pi p),\, \cos(2^0 \pi p),\, \ldots,\, \sin(2^{L-1} \pi p),\, \cos(2^{L-1} \pi p)\right]
$$

This enables the network to learn high-frequency functions.

---

### Implementation

I adopted the recommended architecture: 4 hidden layers, 256 hidden dimensions, and $L = 10$ frequency bands for positional encoding.

---

### Results

<Figure src="/projects/cs180/proj6/NeF_fox_10_256.png" alt="NeF fox 10 256" caption="Neural field reconstruction (Fox, 10 bands, 256 dims)." />
<Figure src="/projects/cs180/proj6/NeF_fox_10_256_psnr.png" alt="NeF fox 10 256 psnr" caption="PSNR over training (Fox, 10 bands, 256 dims)." />

I also trained the neural field on a custom image of my cat.

<Figure src="/projects/cs180/proj6/NeF_cat.png" alt="NeF cat" caption="Neural field reconstruction (Cat)." />
<Figure src="/projects/cs180/proj6/NeF_cat_psnr.png" alt="NeF cat psnr" caption="PSNR over training (Cat)." />

---

### Hyperparameter Tuning

I experimented with varying the number of frequency bands and the hidden dimension size.

Using too few or too many frequency bands degraded reconstruction quality. Using 15 frequency bands produced high-frequency grid-like artifacts.

<ImageGrid>
<Figure src="/projects/cs180/proj6/NeF_fox_4_256.png" alt="NeF fox 4 bands 256 dims" />
<Figure src="/projects/cs180/proj6/NeF_fox_15_256.png" alt="NeF fox 15 bands 256 dims" />
</ImageGrid>

<ImageGrid>
<Figure src="/projects/cs180/proj6/NeF_fox_4_256_psnr.png" alt="NeF fox 4 bands 256 dims PSNR" />
<Figure src="/projects/cs180/proj6/NeF_fox_15_256_psnr.png" alt="NeF fox 15 bands 256 dims PSNR" />
</ImageGrid>

Changing the hidden dimension size had minimal effect on the qualitative results for this 2D task.

<ImageGrid>
<Figure src="/projects/cs180/proj6/NeF_fox_10_128.png" alt="NeF fox 10 bands 128 dims" />
<Figure src="/projects/cs180/proj6/NeF_fox_10_384.png" alt="NeF fox 10 bands 384 dims" />
</ImageGrid>

<ImageGrid>
<Figure src="/projects/cs180/proj6/NeF_fox_10_128_psnr.png" alt="NeF fox 10 bands 128 dims PSNR" />
<Figure src="/projects/cs180/proj6/NeF_fox_10_384_psnr.png" alt="NeF fox 10 bands 384 dims PSNR" />
</ImageGrid>

<SectionDivider />

<a id="nerf" />
## Part 2: Fit a Neural Radiance Field from Multi-view Images

---

<a id="rays" />
### 2.1 Create Rays from Cameras

To train the NeRF, I generated rays corresponding to pixels in the training images using `transform`, `pixel_to_camera`, and `pixel_to_ray` functions with standard coordinate transformations.

---

<a id="sampling-rays" />
### 2.2 Sampling Rays

The `sample_rays` function computes how many rays to sample from each image, selects random pixel coordinates, and generates the corresponding rays ($r_o$, $r_d$) along with their ground truth pixel colors.

---

<a id="dataloading" />
### 2.3 Dataloading

Visualizing the generated rays confirms correct camera geometry and ray generation.

<ImageGrid>
<Figure src="/projects/cs180/proj6/all_rays.png" alt="all rays visualization" />
<Figure src="/projects/cs180/proj6/1_img_rays.png" alt="single image rays" />
<Figure src="/projects/cs180/proj6/top_left_rays.png" alt="top left rays" />
</ImageGrid>

---

<a id="nerf-arch" />
### 2.4 Implementing the NeRF

My final architecture:
- 10-layer MLP + 1 density layer + 3-layer color MLP
- 384 hidden dimensions
- 10-band positional encoding

---

<a id="volume-rendering" />
### 2.5 Volume Rendering

For a ray $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$ with $N$ sample points at distances $t_1, \ldots, t_N$, the rendered color is:

$$
\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i \, \alpha_i \, c_i
$$

where:

$$
\alpha_i = 1 - \exp(-\sigma_i \delta_i), \quad \delta_i = t_{i+1} - t_i
$$

$$
T_i = \prod_{j=1}^{i-1}(1 - \alpha_j)
$$

$\sigma_i$ and $c_i$ are the density and color predicted by the MLP at sample point $i$, $\delta_i$ is the distance between adjacent samples, $\alpha_i$ is the opacity of sample $i$, and $T_i$ is the transmittance (the probability that the ray travels from $t_1$ to $t_i$ without hitting anything).

---

<a id="nerf-results" />
### 2.6 Results

Training configuration:
- 128 samples per ray
- 10,000 rays per step
- 10,000 gradient steps
- Approximately 3 hours on an NVIDIA A1000 GPU

<Figure src="/projects/cs180/proj6/NeRF_training.png" alt="NeRF training" caption="NeRF training curve and sample renders." />

The model reached approximately 30 PSNR on the validation set.

<ImageGrid>
<Figure src="/projects/cs180/proj6/formation.mp4" alt="formation" />
<Figure src="/projects/cs180/proj6/spherical_render.mp4" alt="spherical render" />
</ImageGrid>

<SectionDivider />

<a id="bells-and-whistles" />
## Bells and Whistles: New Background Color

To render the scene against a different background, I modified the volume rendering equation, compositing the final color with a background color $c_{\text{bg}}$ weighted by the remaining transmittance:

$$ \hat{C}(r) = \sum_i T_i \alpha_i c_i + T_i( 1- \alpha_i) c_{\text{bg}} $$

<Figure src="/projects/cs180/proj6/spherical_render_blue.mp4" alt="spherical render blue" caption="Spherical render with blue background." />
