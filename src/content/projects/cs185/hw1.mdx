---
title: "Imitation Learning"
shortTitle: "Imitation Learning"
description: "Action-chunking behavioral cloning with MSE and flow matching policies for the Push-T environment."
thumbnail: "/projects/cs185/hw1/flow_curves.png"
heroImage: "/projects/cs185/hw1/flow_curves.png"
gitUrl: "https://github.com/ozanbayiz/cs185hw1"
date: "2026-02-11"
---

<SectionDivider />

<a id="setup" />
## Problem Setup

The environment is Push-T, a 2D control task where an agent pushes a T-shaped block into a goal zone.
The observation is a 5-dimensional state vector containing the positions of the T-block and the agent.
The action is a 2-dimensional target position for the agent.

Both policies use action chunking: at each decision step, the policy predicts $K = 8$ consecutive actions and executes them open-loop. Given an observation $\mathbf{o} \in \mathbb{R}^5$, the policy outputs an action chunk $\mathbf{A} \in \mathbb{R}^{K \times 2}$.

The training dataset $\mathcal{D} = \{(\mathbf{o}_i, \mathbf{A}_i)\}$ consists of expert demonstrations collected in the Push-T environment. States and actions are z-score normalized per feature before training.

---

<a id="mse" />
## MSE Policy

### Method

The MSE policy is a behavioral cloning model that predicts a single action chunk per observation by minimizing the mean squared error against the expert action chunk:

$$
\mathcal{L}_{\text{MSE}} = \mathbb{E}_{(\mathbf{o}, \mathbf{A}) \sim \mathcal{D}} \left[ \| f_\theta(\mathbf{o}) - \mathbf{A} \|^2 \right]
$$

where $f_\theta : \mathbb{R}^5 \to \mathbb{R}^{K \times 2}$ is the policy network.

### Architecture

I implemented $f_\theta$ as a 3-layer MLP with hidden dimensions $(256, 256, 256)$ and ReLU activations. The 5-dimensional state maps to a flat vector of size $K \times 2 = 16$, which is reshaped to $K = 8$ two-dimensional actions. I trained with Adam (learning rate $3 \times 10^{-4}$, no weight decay) for 400 epochs.

---

<a id="flow" />Â¬
## Flow Matching Policy

### Method

Flow matching defines a conditional probability path from a noise distribution to the expert action distribution. For each expert action chunk $\mathbf{A}_1$ and a noise sample $\mathbf{A}_0 \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$, the interpolated sample at time $\tau \in [0, 1]$ is:

$$
\mathbf{A}_\tau = \tau \, \mathbf{A}_1 + (1 - \tau) \, \mathbf{A}_0
$$

The velocity along this path is:

$$
\mathbf{v}_\tau = \frac{d\mathbf{A}_\tau}{d\tau} = \mathbf{A}_1 - \mathbf{A}_0
$$

A neural network $\mathbf{v}_\theta(\mathbf{o}, \mathbf{A}_\tau, \tau)$ is trained to predict this velocity. The training loss is:

$$
\mathcal{L}_{\text{FM}} = \mathbb{E}_{(\mathbf{o}, \mathbf{A}_1) \sim \mathcal{D},\; \mathbf{A}_0 \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),\; \tau \sim \mathcal{U}[0,1]} \left[ \left\| \mathbf{v}_\theta(\mathbf{o}, \mathbf{A}_\tau, \tau) - (\mathbf{A}_1 - \mathbf{A}_0) \right\|^2 \right]
$$

At inference time, action chunks are generated by solving the ODE from $\tau = 0$ to $\tau = 1$ with Euler integration:

$$
\mathbf{A}_{\tau + \Delta\tau} = \mathbf{A}_\tau + \Delta\tau \cdot \mathbf{v}_\theta(\mathbf{o}, \mathbf{A}_\tau, \tau), \quad \Delta\tau = \frac{1}{n}
$$

starting from $\mathbf{A}_0 \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ with $n = 10$ integration steps.

### Architecture

I used the same 3-layer MLP with hidden dimensions $(256, 256, 256)$ and ReLU activations. The input is the observation concatenated with the flattened noisy action chunk and the scalar timestep $\tau$, giving an input dimension of $5 + 16 + 1 = 22$. Training used the same optimizer and schedule as the MSE policy.

---

<a id="results" />
## Results

### Training Curves

<Figure src="/projects/cs185/hw1/comparison.png" alt="MSE vs flow matching reward comparison" caption="Evaluation reward over training: MSE policy (left) vs. flow matching policy (right)." />

<ImageGrid>
<Figure src="/projects/cs185/hw1/mse_loss.png" alt="MSE policy training loss" caption="MSE policy: smoothed training loss." />
<Figure src="/projects/cs185/hw1/flow_loss.png" alt="Flow matching policy training loss" caption="Flow matching policy: smoothed training loss." />
</ImageGrid>

| Policy | Final Eval Reward | Threshold |
|---|---|---|
| MSE | ~0.61 | 0.50 |
| Flow Matching | ~0.84 | 0.70 |

### Rollouts: Early (20k steps)

<ImageGrid>
<Figure src="/projects/cs185/hw1/mse_rollout_early.mp4" alt="MSE rollout at 20k steps" caption="MSE policy at 20k steps." />
<Figure src="/projects/cs185/hw1/flow_rollout_early.mp4" alt="Flow rollout at 20k steps" caption="Flow matching policy at 20k steps." />
</ImageGrid>

### Rollouts: Peak Performance

<ImageGrid>
<Figure src="/projects/cs185/hw1/mse_rollout_0.mp4" alt="MSE rollout at 60k steps" caption="MSE policy at 60k steps (peak, reward ~0.64)." />
<Figure src="/projects/cs185/hw1/flow_rollout_0.mp4" alt="Flow rollout at 70k steps" caption="Flow matching policy at 70k steps (peak, reward ~0.86)." />
</ImageGrid>

### Analysis

The MSE policy learns $\mathbb{E}[\mathbf{A} \mid \mathbf{o}]$, which averages over all modes of the action distribution. The flow matching policy samples from the full conditional $p(\mathbf{A} \mid \mathbf{o})$, producing trajectories that stay within a single mode. This difference is visible in the rollouts: the flow agent maintains sustained contact with the T-block and pushes it into the goal region more consistently.
